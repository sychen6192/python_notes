{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 找出觀察的最近鄰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data # 不用target\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二最近鄰\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=2).fit(features_standardized)\n",
    "new_observation = [1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.03800476, 0.55861082, 1.10378283, 1.18556721],\n",
       "        [0.79566902, 0.32841405, 0.76275827, 1.05393502]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出距離與觀察之最近鄰點的索引\n",
    "distances, indices = nearest_neighbors.kneighbors([new_observation])\n",
    "features_standardized[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d_{euclidean}=\\sqrt{\\sum_{i=1}^n(x_i-y_i)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d_{manhattan}=\\sum_{i=1}^n|x_i-y_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在預測的情況下，NearestNeighbors使用的事閔可夫司基距離(Minkowski distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$d_{minkowski}=(\\sum_{i=1}^n|x_i-y_i|^p)^{1/p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依據歐幾里德距離找出二最近鄰\n",
    "nearestneighbors_euclidean = NearestNeighbors(n_neighbors=2, metric='euclidean').fit(features_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49140089, 0.74294782]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 依據歐幾里德距離找出三最近鄰\n",
    "nearestneighbors_euclidean = NearestNeighbors(n_neighbors=3, metric='euclidean').fit(features_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 列出每個觀察的三個最近鄰(包含自己)\n",
    "nearest_neighbors_with_self = nearestneighbors_euclidean.kneighbors_graph(features_standardized).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 移除讓自己成為最近觀察的1\n",
    "for i, x in enumerate(nearest_neighbors_with_self):\n",
    "    x[i] = 0\n",
    "nearest_neighbors_with_self[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN分類器製作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以五個鄰點訓練KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "knn.fit(X_std, y)\n",
    "new_observation = [[0.75, 0.75, 0.75, 0.75], [1, 1, 1, 1]]\n",
    "knn.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: 在KNN中，給定一觀察$x_u$，其目標類未知，演算法會依照某距離指標，先找到k個最接近的觀察(有時稱為$x_u$的鄰點)，然後再由這k個觀察再依據自己所屬類型投票選出$x_u$的預測類型。正式地來說，$x_u$為某類型$j$的機率是：\n",
    "$$\\frac{1}{k}\\sum_{i \\in v}I(y_i=j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0. , 0.6, 0.4],\n",
       "       [0. , 0. , 1. ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 參數說明：\n",
    "**metric**：設定距離量測指標。<br>\n",
    "**n_jobs**：設定使用多少電腦運算核心。<br>\n",
    "**algorithm**：設定用來計算最近鄰點的方法。KNNCLF通常會挑最好的來做。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 找出最近鄰大小(n_neighbors=?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "pipe = Pipeline([(\"scaler\", scaler), (\"knn\", knn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 產生候選值\n",
    "search_space = [{\"knn__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}]\n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0).fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results 最佳近鄰大小K\n",
    "clf.best_estimator_.get_params()[\"knn__n_neighbors\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: 目的在於k值大小會影響到KNN分類器，而在學習當中，我們要試著去權衡bias與variance。例如：k=n，其中n為觀察數，則會有高bias與低variance。若k=1則會有低bias與高variance。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 半徑型NN分類器製作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "scaler = StandardScaler()\n",
    "feature_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練半徑NN分類器\n",
    "rnn = RadiusNeighborsClassifier(radius=.5, n_jobs=-1)\n",
    "rnn.fit(feature_standardized, target)\n",
    "new_observation = [[1, 1, 1, 1]]\n",
    "rnn.predict(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16 邏輯迴歸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = datasets.load_iris()\n",
    "# 只載入二類型的資料\n",
    "features, target = iris.data[:100, :], iris.target[:100]\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(solver=\"lbfgs\", random_state=0)\n",
    "model = lr.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: 在邏輯回歸中，會有一個線性模型(即$\\beta_0+\\beta_1x)$)包含在邏輯函式，$\\frac{1}{1+e^{-z}}$中，即$P(y_i=1|X)=\\frac{1}{1+e^-(\\beta_0+\\beta_1x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_observation = [[.5, .5, .5, .5]]\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17738424, 0.82261576]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我們的觀察有18%的機率是類型0，有82%是類型1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 訓練多類型分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"lbfgs\", random_state=0, multi_class=\"ovr\") # 需要加上此參數\n",
    "model = lr.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "**OVR**：OVR為一對其他的邏輯迴歸，其中會有個別為每一類型訓練的模型，可預測一觀察是不是屬於該類型(變成二元了)。<br>\n",
    "**MLR**：MLR的邏輯函式會被替換成歸一化指數函式(softmax function) <params: multi_class=\"multunomial\"><br>\n",
    "$$P(y_i=k|X)=\\frac{e^{\\beta_kx_i}}{\\sum_{j=1}^{K}e^{\\beta_jx_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以正規化減少變異"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 調整正規化強度超參數C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "scaler = StandardScaler()\n",
    "feature_standardized = scaler.fit_transform(features)\n",
    "lr = LogisticRegressionCV(penalty=\"l2\", Cs=10, # Cs為C的搜尋範圍\n",
    "                          random_state=0, n_jobs=-1, solver=\"lbfgs\", multi_class=\"ovr\", cv=5)\n",
    "model = lr.fit(feature_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: 正規化是一種懲罰複雜模型以降低其變異的方法。明確的說，會有一個懲罰項加進要做最小化的loss function中，通常是L1, L2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L1=\\alpha\\sum_{j=1}^p|\\hat{\\beta_j}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L2=\\alpha\\sum_{j=1}^p\\hat{\\beta_j}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha$越大複雜的模型懲罰越大，scikit-learn依循較普遍的做法，即使用C來代替$\\alpha$，$C=\\frac{1}{\\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超大資料集在分類器的訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data, iris.target\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 這邊使用隨機平均梯度(stochastic average gradient，SAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"sag\", random_state=0, multi_class=\"ovr\") # solver的改變你看得見\n",
    "model = lr.fit(feature_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 處理不平衡的分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "iris = datasets.load_iris()\n",
    "features, target = iris.data[40:, :], iris.target[40:]\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.where((target==0), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\", random_state=0, class_weight=\"balanced\")\n",
    "model = lr.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: balanced參數能自動地為類型以與其出現次數成反比的方式加權：$w_j=\\frac{n}{kn_j}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
