{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 名目類型特徵編碼(LabelBinarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "feature = np.array([[\"Texas\"], [\"California\"], [\"Texas\"], [\"Delaware\"], [\"Texas\"]])\n",
    "one_hot = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用one_hot 做特徵編碼\n",
    "one_hot.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['California', 'Delaware', 'Texas'], dtype='<U10')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 檢視特徵類型\n",
    "one_hot.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Texas', 'California', 'Texas', 'Delaware', 'Texas'], dtype='<U10')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 反向編碼\n",
    "one_hot.inverse_transform(one_hot.fit_transform(feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 甚至可以使用Pandas做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Texas'],\n",
       "       ['California'],\n",
       "       ['Texas'],\n",
       "       ['Delaware'],\n",
       "       ['Texas']], dtype='<U10')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>California</th>\n",
       "      <th>Delaware</th>\n",
       "      <th>Texas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   California  Delaware  Texas\n",
       "0           0         0      1\n",
       "1           1         0      0\n",
       "2           0         0      1\n",
       "3           0         1      0\n",
       "4           0         0      1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.get_dummies(feature[:, 0]) # 出來直接是一個DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多重特徵編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "muticlass_feature = [(\"Texas\", \"Florida\"),\n",
    "                    (\"California\", \"Alabama\"),\n",
    "                    (\"Texas\", \"Florida\"),\n",
    "                    (\"Delware\", \"Florida\"),\n",
    "                    (\"Texas\", \"Alabama\")]\n",
    "one_hot_multicalass = MultiLabelBinarizer()\n",
    "one_hot_multicalass.fit_transform(muticlass_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alabama', 'California', 'Delware', 'Florida', 'Texas'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multicalass.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  有序類型特徵編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    2\n",
       "3    2\n",
       "4    3\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"Score\":[\"Low\", \"Low\", \"Medium\", \"Medium\", \"High\"]})\n",
    "scale_mapper = {\"Low\": 1, \"Medium\":2, \"High\":3 }\n",
    "df[\"Score\"].replace(scale_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特徵字典編碼 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 2., 0.],\n",
       "       [3., 4., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 2.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "data_dict = [{\"Red\":2, \"Blue\":4}, {\"Red\":4, \"Blue\":3}, {\"Red\":1, \"Yellow\":2}, {\"Red\":2, \"Yellow\":2}]\n",
    "dictvectorizer = DictVectorizer(sparse=False) # Dense matrix 降低記憶體需求\n",
    "features = dictvectorizer.fit_transform(data_dict)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blue', 'Red', 'Yellow']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取得特徵名稱\n",
    "feature_names = dictvectorizer.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Blue</th>\n",
       "      <th>Red</th>\n",
       "      <th>Yellow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Blue  Red  Yellow\n",
       "0   4.0  2.0     0.0\n",
       "1   3.0  4.0     0.0\n",
       "2   0.0  1.0     2.0\n",
       "3   0.0  2.0     2.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(features, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 2., 0.],\n",
       "       [3., 4., 0.],\n",
       "       [0., 1., 2.],\n",
       "       [0., 2., 2.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 假設有四份文件(NLP)\n",
    "doc_1 = {\"Red\":2, \"Blue\":4}\n",
    "doc_2 = {\"Red\":4, \"Blue\":3}\n",
    "doc_3 = {\"Red\":1, \"Yellow\":2}\n",
    "doc_4 = {\"Red\":2, \"Yellow\":2}\n",
    "doc_word_counts = [doc_1, doc_2, doc_3, doc_4]\n",
    "dictvectorizer.fit_transform(doc_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缺漏類型值的推算 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 1.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = np.array([[0, 2.10, 1.45],\n",
    "              [1, 1.18, 1.33],\n",
    "              [0, 1.22, 1.27],\n",
    "              [1,-0.21,-1.19]])\n",
    "X_with_nan = np.array([[np.nan, 0.87, 1.31],\n",
    "                       [np.nan,-0.67,-0.22]])\n",
    "clf = KNeighborsClassifier(3, weights ='distance')\n",
    "trained_model = clf.fit(X[:, 1:], X[:, 0])\n",
    "# 預測模型\n",
    "imputed_values = trained_model.predict(X_with_nan[:, 1:])\n",
    "# 合成預測值跟原始\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1, 1), X_with_nan[:, 1:]))\n",
    "# 兩個矩陣[X_with_nan, X]結合\n",
    "np.vstack((X_with_imputed, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用特徵頻率最高的值來補空值 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  1.31],\n",
       "       [ 0.  , -0.67, -0.22],\n",
       "       [ 0.  ,  2.1 ,  1.45],\n",
       "       [ 1.  ,  1.18,  1.33],\n",
       "       [ 0.  ,  1.22,  1.27],\n",
       "       [ 1.  , -0.21, -1.19]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "X_complete = np.vstack((X_with_nan, X)) # 含空值的合併\n",
    "imputer = Imputer(strategy='most_frequent', axis=0)\n",
    "imputer.fit_transform(X_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:最好使用機器學習演算法來預測及填補資料，最常用KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 處理高度不平衡類型的目標向量 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成資料\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = features[40:, :]\n",
    "target = target[40:]\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.where((target==0), 0, 1)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight={0: 0.9, 1: 0.1},\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators='warn', n_jobs=None, oob_score=False,\n",
       "                       random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 產生權重(?)\n",
    "weights={0: .9, 1: 0.1}\n",
    "RandomForestClassifier(class_weight=weights)\n",
    "# 自動產生死類型出現頻率成反比的權重\n",
    "# RandomForestClassifier(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:或者我們可以向下取樣，針對多數類型進行隨機取樣已產生相當於少數類型的新子集 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向下取樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64),)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 跟下面比較看看哪裡不同\n",
    "i_class0 = np.where(target==0)\n",
    "i_class0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_class0 = np.where(target==0)[0]\n",
    "i_class1 = np.where(target==1)[0]\n",
    "i_class0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 100\n"
     ]
    }
   ],
   "source": [
    "print(len(i_class0), len(i_class1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_class1_downsampled = np.random.choice(i_class1, size=len(i_class0), replace=False) # replace=False取後不放回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((target[i_class0], target[i_class1_downsampled]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack((features[i_class0, :], features[i_class1_downsampled, :]))[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向上取樣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 3.5, 1.6, 0.6],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_class0_upsampled = np.random.choice(i_class0, size=len(i_class1), replace=True) # replace=True取後放回\n",
    "np.hstack((target[i_class0_upsampled], target[i_class1]))\n",
    "# 也可以用concatenate!\n",
    "# np.concatenate((target[i_class0_upsampled], target[i_class1]))\n",
    "np.vstack((features[i_class0_upsampled, :], features[i_class1, :]))[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 處理不平衡的策略 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 增加數據\n",
    "2. 使用模型評估指標(Confusion Matrix, F1-score, ROC curves)\n",
    "3. 使用類型權重參數，(class_weight=?)\n",
    "4. 向上或是向下取樣(通常兩種都會試)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   Hello. By Shao Yun   ',\n",
       " 'Parking And Gaming. By Hong Lee',\n",
       " '    Today Is the night. By Zhao Zhoo']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 範例\n",
    "text_data = [\"   Hello. By Shao Yun   \",\n",
    "             \"Parking And Gaming. By Hong Lee\",\n",
    "             \"    Today Is the night. By Zhao Zhoo\"]\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello. By Shao Yun',\n",
       " 'Parking And Gaming. By Hong Lee',\n",
       " 'Today Is the night. By Zhao Zhoo']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 除去前後空格\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello By Shao Yun',\n",
       " 'Parking And Gaming By Hong Lee',\n",
       " 'Today Is the night By Zhao Zhoo']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 移除句點\n",
    "remove_periods = [string.replace(\".\", \"\") for string in strip_whitespace]\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalizer(string: str) -> str: # function annotation.`\n",
    "    return string.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HELLO BY SHAO YUN',\n",
       " 'PARKING AND GAMING BY HONG LEE',\n",
       " 'TODAY IS THE NIGHT BY ZHAO ZHOO']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXX XX XXXX XXX',\n",
       " 'XXXXXXX XXX XXXXXX XX XXXX XXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXX XXXX']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regex example\n",
    "import re\n",
    "def replace_letters_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\", \"X\", string) # [a-z][A-Z]≠[a-zA-Z]\n",
    "[replace_letters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML 解析與清理 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"\"\"\n",
    "        <div class ='full_name'><span style='font-weight:bold'>Masego</span> Azra</div>\n",
    "          \"\"\"\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Masego Azra'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"div\", {\"class\": \"full_name\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標點符號的移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIT', 'Right']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "text_data = [\"Hi!!!! I. Love. This Song....\", \"10000% Agree!!!! #LoveIT\", \"Right?!?!\"]\n",
    "# 產生標點符號字元字典\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "# 將每個字串標點符號移除\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 95: None,\n",
       " 123: None,\n",
       " 125: None,\n",
       " 161: None,\n",
       " 167: None,\n",
       " 171: None,\n",
       " 182: None,\n",
       " 183: None,\n",
       " 187: None,\n",
       " 191: None,\n",
       " 894: None,\n",
       " 903: None,\n",
       " 1370: None,\n",
       " 1371: None,\n",
       " 1372: None,\n",
       " 1373: None,\n",
       " 1374: None,\n",
       " 1375: None,\n",
       " 1417: None,\n",
       " 1418: None,\n",
       " 1470: None,\n",
       " 1472: None,\n",
       " 1475: None,\n",
       " 1478: None,\n",
       " 1523: None,\n",
       " 1524: None,\n",
       " 1545: None,\n",
       " 1546: None,\n",
       " 1548: None,\n",
       " 1549: None,\n",
       " 1563: None,\n",
       " 1566: None,\n",
       " 1567: None,\n",
       " 1642: None,\n",
       " 1643: None,\n",
       " 1644: None,\n",
       " 1645: None,\n",
       " 1748: None,\n",
       " 1792: None,\n",
       " 1793: None,\n",
       " 1794: None,\n",
       " 1795: None,\n",
       " 1796: None,\n",
       " 1797: None,\n",
       " 1798: None,\n",
       " 1799: None,\n",
       " 1800: None,\n",
       " 1801: None,\n",
       " 1802: None,\n",
       " 1803: None,\n",
       " 1804: None,\n",
       " 1805: None,\n",
       " 2039: None,\n",
       " 2040: None,\n",
       " 2041: None,\n",
       " 2096: None,\n",
       " 2097: None,\n",
       " 2098: None,\n",
       " 2099: None,\n",
       " 2100: None,\n",
       " 2101: None,\n",
       " 2102: None,\n",
       " 2103: None,\n",
       " 2104: None,\n",
       " 2105: None,\n",
       " 2106: None,\n",
       " 2107: None,\n",
       " 2108: None,\n",
       " 2109: None,\n",
       " 2110: None,\n",
       " 2142: None,\n",
       " 2404: None,\n",
       " 2405: None,\n",
       " 2416: None,\n",
       " 2557: None,\n",
       " 2678: None,\n",
       " 2800: None,\n",
       " 3204: None,\n",
       " 3572: None,\n",
       " 3663: None,\n",
       " 3674: None,\n",
       " 3675: None,\n",
       " 3844: None,\n",
       " 3845: None,\n",
       " 3846: None,\n",
       " 3847: None,\n",
       " 3848: None,\n",
       " 3849: None,\n",
       " 3850: None,\n",
       " 3851: None,\n",
       " 3852: None,\n",
       " 3853: None,\n",
       " 3854: None,\n",
       " 3855: None,\n",
       " 3856: None,\n",
       " 3857: None,\n",
       " 3858: None,\n",
       " 3860: None,\n",
       " 3898: None,\n",
       " 3899: None,\n",
       " 3900: None,\n",
       " 3901: None,\n",
       " 3973: None,\n",
       " 4048: None,\n",
       " 4049: None,\n",
       " 4050: None,\n",
       " 4051: None,\n",
       " 4052: None,\n",
       " 4057: None,\n",
       " 4058: None,\n",
       " 4170: None,\n",
       " 4171: None,\n",
       " 4172: None,\n",
       " 4173: None,\n",
       " 4174: None,\n",
       " 4175: None,\n",
       " 4347: None,\n",
       " 4960: None,\n",
       " 4961: None,\n",
       " 4962: None,\n",
       " 4963: None,\n",
       " 4964: None,\n",
       " 4965: None,\n",
       " 4966: None,\n",
       " 4967: None,\n",
       " 4968: None,\n",
       " 5120: None,\n",
       " 5741: None,\n",
       " 5742: None,\n",
       " 5787: None,\n",
       " 5788: None,\n",
       " 5867: None,\n",
       " 5868: None,\n",
       " 5869: None,\n",
       " 5941: None,\n",
       " 5942: None,\n",
       " 6100: None,\n",
       " 6101: None,\n",
       " 6102: None,\n",
       " 6104: None,\n",
       " 6105: None,\n",
       " 6106: None,\n",
       " 6144: None,\n",
       " 6145: None,\n",
       " 6146: None,\n",
       " 6147: None,\n",
       " 6148: None,\n",
       " 6149: None,\n",
       " 6150: None,\n",
       " 6151: None,\n",
       " 6152: None,\n",
       " 6153: None,\n",
       " 6154: None,\n",
       " 6468: None,\n",
       " 6469: None,\n",
       " 6686: None,\n",
       " 6687: None,\n",
       " 6816: None,\n",
       " 6817: None,\n",
       " 6818: None,\n",
       " 6819: None,\n",
       " 6820: None,\n",
       " 6821: None,\n",
       " 6822: None,\n",
       " 6824: None,\n",
       " 6825: None,\n",
       " 6826: None,\n",
       " 6827: None,\n",
       " 6828: None,\n",
       " 6829: None,\n",
       " 7002: None,\n",
       " 7003: None,\n",
       " 7004: None,\n",
       " 7005: None,\n",
       " 7006: None,\n",
       " 7007: None,\n",
       " 7008: None,\n",
       " 7164: None,\n",
       " 7165: None,\n",
       " 7166: None,\n",
       " 7167: None,\n",
       " 7227: None,\n",
       " 7228: None,\n",
       " 7229: None,\n",
       " 7230: None,\n",
       " 7231: None,\n",
       " 7294: None,\n",
       " 7295: None,\n",
       " 7360: None,\n",
       " 7361: None,\n",
       " 7362: None,\n",
       " 7363: None,\n",
       " 7364: None,\n",
       " 7365: None,\n",
       " 7366: None,\n",
       " 7367: None,\n",
       " 7379: None,\n",
       " 8208: None,\n",
       " 8209: None,\n",
       " 8210: None,\n",
       " 8211: None,\n",
       " 8212: None,\n",
       " 8213: None,\n",
       " 8214: None,\n",
       " 8215: None,\n",
       " 8216: None,\n",
       " 8217: None,\n",
       " 8218: None,\n",
       " 8219: None,\n",
       " 8220: None,\n",
       " 8221: None,\n",
       " 8222: None,\n",
       " 8223: None,\n",
       " 8224: None,\n",
       " 8225: None,\n",
       " 8226: None,\n",
       " 8227: None,\n",
       " 8228: None,\n",
       " 8229: None,\n",
       " 8230: None,\n",
       " 8231: None,\n",
       " 8240: None,\n",
       " 8241: None,\n",
       " 8242: None,\n",
       " 8243: None,\n",
       " 8244: None,\n",
       " 8245: None,\n",
       " 8246: None,\n",
       " 8247: None,\n",
       " 8248: None,\n",
       " 8249: None,\n",
       " 8250: None,\n",
       " 8251: None,\n",
       " 8252: None,\n",
       " 8253: None,\n",
       " 8254: None,\n",
       " 8255: None,\n",
       " 8256: None,\n",
       " 8257: None,\n",
       " 8258: None,\n",
       " 8259: None,\n",
       " 8261: None,\n",
       " 8262: None,\n",
       " 8263: None,\n",
       " 8264: None,\n",
       " 8265: None,\n",
       " 8266: None,\n",
       " 8267: None,\n",
       " 8268: None,\n",
       " 8269: None,\n",
       " 8270: None,\n",
       " 8271: None,\n",
       " 8272: None,\n",
       " 8273: None,\n",
       " 8275: None,\n",
       " 8276: None,\n",
       " 8277: None,\n",
       " 8278: None,\n",
       " 8279: None,\n",
       " 8280: None,\n",
       " 8281: None,\n",
       " 8282: None,\n",
       " 8283: None,\n",
       " 8284: None,\n",
       " 8285: None,\n",
       " 8286: None,\n",
       " 8317: None,\n",
       " 8318: None,\n",
       " 8333: None,\n",
       " 8334: None,\n",
       " 8968: None,\n",
       " 8969: None,\n",
       " 8970: None,\n",
       " 8971: None,\n",
       " 9001: None,\n",
       " 9002: None,\n",
       " 10088: None,\n",
       " 10089: None,\n",
       " 10090: None,\n",
       " 10091: None,\n",
       " 10092: None,\n",
       " 10093: None,\n",
       " 10094: None,\n",
       " 10095: None,\n",
       " 10096: None,\n",
       " 10097: None,\n",
       " 10098: None,\n",
       " 10099: None,\n",
       " 10100: None,\n",
       " 10101: None,\n",
       " 10181: None,\n",
       " 10182: None,\n",
       " 10214: None,\n",
       " 10215: None,\n",
       " 10216: None,\n",
       " 10217: None,\n",
       " 10218: None,\n",
       " 10219: None,\n",
       " 10220: None,\n",
       " 10221: None,\n",
       " 10222: None,\n",
       " 10223: None,\n",
       " 10627: None,\n",
       " 10628: None,\n",
       " 10629: None,\n",
       " 10630: None,\n",
       " 10631: None,\n",
       " 10632: None,\n",
       " 10633: None,\n",
       " 10634: None,\n",
       " 10635: None,\n",
       " 10636: None,\n",
       " 10637: None,\n",
       " 10638: None,\n",
       " 10639: None,\n",
       " 10640: None,\n",
       " 10641: None,\n",
       " 10642: None,\n",
       " 10643: None,\n",
       " 10644: None,\n",
       " 10645: None,\n",
       " 10646: None,\n",
       " 10647: None,\n",
       " 10648: None,\n",
       " 10712: None,\n",
       " 10713: None,\n",
       " 10714: None,\n",
       " 10715: None,\n",
       " 10748: None,\n",
       " 10749: None,\n",
       " 11513: None,\n",
       " 11514: None,\n",
       " 11515: None,\n",
       " 11516: None,\n",
       " 11518: None,\n",
       " 11519: None,\n",
       " 11632: None,\n",
       " 11776: None,\n",
       " 11777: None,\n",
       " 11778: None,\n",
       " 11779: None,\n",
       " 11780: None,\n",
       " 11781: None,\n",
       " 11782: None,\n",
       " 11783: None,\n",
       " 11784: None,\n",
       " 11785: None,\n",
       " 11786: None,\n",
       " 11787: None,\n",
       " 11788: None,\n",
       " 11789: None,\n",
       " 11790: None,\n",
       " 11791: None,\n",
       " 11792: None,\n",
       " 11793: None,\n",
       " 11794: None,\n",
       " 11795: None,\n",
       " 11796: None,\n",
       " 11797: None,\n",
       " 11798: None,\n",
       " 11799: None,\n",
       " 11800: None,\n",
       " 11801: None,\n",
       " 11802: None,\n",
       " 11803: None,\n",
       " 11804: None,\n",
       " 11805: None,\n",
       " 11806: None,\n",
       " 11807: None,\n",
       " 11808: None,\n",
       " 11809: None,\n",
       " 11810: None,\n",
       " 11811: None,\n",
       " 11812: None,\n",
       " 11813: None,\n",
       " 11814: None,\n",
       " 11815: None,\n",
       " 11816: None,\n",
       " 11817: None,\n",
       " 11818: None,\n",
       " 11819: None,\n",
       " 11820: None,\n",
       " 11821: None,\n",
       " 11822: None,\n",
       " 11824: None,\n",
       " 11825: None,\n",
       " 11826: None,\n",
       " 11827: None,\n",
       " 11828: None,\n",
       " 11829: None,\n",
       " 11830: None,\n",
       " 11831: None,\n",
       " 11832: None,\n",
       " 11833: None,\n",
       " 11834: None,\n",
       " 11835: None,\n",
       " 11836: None,\n",
       " 11837: None,\n",
       " 11838: None,\n",
       " 11839: None,\n",
       " 11840: None,\n",
       " 11841: None,\n",
       " 11842: None,\n",
       " 11843: None,\n",
       " 11844: None,\n",
       " 11845: None,\n",
       " 11846: None,\n",
       " 11847: None,\n",
       " 11848: None,\n",
       " 11849: None,\n",
       " 11850: None,\n",
       " 11851: None,\n",
       " 11852: None,\n",
       " 11853: None,\n",
       " 11854: None,\n",
       " 12289: None,\n",
       " 12290: None,\n",
       " 12291: None,\n",
       " 12296: None,\n",
       " 12297: None,\n",
       " 12298: None,\n",
       " 12299: None,\n",
       " 12300: None,\n",
       " 12301: None,\n",
       " 12302: None,\n",
       " 12303: None,\n",
       " 12304: None,\n",
       " 12305: None,\n",
       " 12308: None,\n",
       " 12309: None,\n",
       " 12310: None,\n",
       " 12311: None,\n",
       " 12312: None,\n",
       " 12313: None,\n",
       " 12314: None,\n",
       " 12315: None,\n",
       " 12316: None,\n",
       " 12317: None,\n",
       " 12318: None,\n",
       " 12319: None,\n",
       " 12336: None,\n",
       " 12349: None,\n",
       " 12448: None,\n",
       " 12539: None,\n",
       " 42238: None,\n",
       " 42239: None,\n",
       " 42509: None,\n",
       " 42510: None,\n",
       " 42511: None,\n",
       " 42611: None,\n",
       " 42622: None,\n",
       " 42738: None,\n",
       " 42739: None,\n",
       " 42740: None,\n",
       " 42741: None,\n",
       " 42742: None,\n",
       " 42743: None,\n",
       " 43124: None,\n",
       " 43125: None,\n",
       " 43126: None,\n",
       " 43127: None,\n",
       " 43214: None,\n",
       " 43215: None,\n",
       " 43256: None,\n",
       " 43257: None,\n",
       " 43258: None,\n",
       " 43260: None,\n",
       " 43310: None,\n",
       " 43311: None,\n",
       " 43359: None,\n",
       " 43457: None,\n",
       " 43458: None,\n",
       " 43459: None,\n",
       " 43460: None,\n",
       " 43461: None,\n",
       " 43462: None,\n",
       " 43463: None,\n",
       " 43464: None,\n",
       " 43465: None,\n",
       " 43466: None,\n",
       " 43467: None,\n",
       " 43468: None,\n",
       " 43469: None,\n",
       " 43486: None,\n",
       " 43487: None,\n",
       " 43612: None,\n",
       " 43613: None,\n",
       " 43614: None,\n",
       " 43615: None,\n",
       " 43742: None,\n",
       " 43743: None,\n",
       " 43760: None,\n",
       " 43761: None,\n",
       " 44011: None,\n",
       " 64830: None,\n",
       " 64831: None,\n",
       " 65040: None,\n",
       " 65041: None,\n",
       " 65042: None,\n",
       " 65043: None,\n",
       " 65044: None,\n",
       " 65045: None,\n",
       " 65046: None,\n",
       " 65047: None,\n",
       " 65048: None,\n",
       " 65049: None,\n",
       " 65072: None,\n",
       " 65073: None,\n",
       " 65074: None,\n",
       " 65075: None,\n",
       " 65076: None,\n",
       " 65077: None,\n",
       " 65078: None,\n",
       " 65079: None,\n",
       " 65080: None,\n",
       " 65081: None,\n",
       " 65082: None,\n",
       " 65083: None,\n",
       " 65084: None,\n",
       " 65085: None,\n",
       " 65086: None,\n",
       " 65087: None,\n",
       " 65088: None,\n",
       " 65089: None,\n",
       " 65090: None,\n",
       " 65091: None,\n",
       " 65092: None,\n",
       " 65093: None,\n",
       " 65094: None,\n",
       " 65095: None,\n",
       " 65096: None,\n",
       " 65097: None,\n",
       " 65098: None,\n",
       " 65099: None,\n",
       " 65100: None,\n",
       " 65101: None,\n",
       " 65102: None,\n",
       " 65103: None,\n",
       " 65104: None,\n",
       " 65105: None,\n",
       " 65106: None,\n",
       " 65108: None,\n",
       " 65109: None,\n",
       " 65110: None,\n",
       " 65111: None,\n",
       " 65112: None,\n",
       " 65113: None,\n",
       " 65114: None,\n",
       " 65115: None,\n",
       " 65116: None,\n",
       " 65117: None,\n",
       " 65118: None,\n",
       " 65119: None,\n",
       " 65120: None,\n",
       " 65121: None,\n",
       " 65123: None,\n",
       " 65128: None,\n",
       " 65130: None,\n",
       " 65131: None,\n",
       " 65281: None,\n",
       " 65282: None,\n",
       " 65283: None,\n",
       " 65285: None,\n",
       " 65286: None,\n",
       " 65287: None,\n",
       " 65288: None,\n",
       " 65289: None,\n",
       " 65290: None,\n",
       " 65292: None,\n",
       " 65293: None,\n",
       " 65294: None,\n",
       " 65295: None,\n",
       " 65306: None,\n",
       " 65307: None,\n",
       " 65311: None,\n",
       " 65312: None,\n",
       " 65339: None,\n",
       " 65340: None,\n",
       " 65341: None,\n",
       " 65343: None,\n",
       " 65371: None,\n",
       " 65373: None,\n",
       " 65375: None,\n",
       " 65376: None,\n",
       " 65377: None,\n",
       " 65378: None,\n",
       " 65379: None,\n",
       " 65380: None,\n",
       " 65381: None,\n",
       " 65792: None,\n",
       " 65793: None,\n",
       " 65794: None,\n",
       " 66463: None,\n",
       " 66512: None,\n",
       " 66927: None,\n",
       " 67671: None,\n",
       " 67871: None,\n",
       " 67903: None,\n",
       " 68176: None,\n",
       " 68177: None,\n",
       " 68178: None,\n",
       " 68179: None,\n",
       " 68180: None,\n",
       " 68181: None,\n",
       " 68182: None,\n",
       " 68183: None,\n",
       " 68184: None,\n",
       " 68223: None,\n",
       " 68336: None,\n",
       " 68337: None,\n",
       " 68338: None,\n",
       " 68339: None,\n",
       " 68340: None,\n",
       " 68341: None,\n",
       " 68342: None,\n",
       " 68409: None,\n",
       " 68410: None,\n",
       " 68411: None,\n",
       " 68412: None,\n",
       " 68413: None,\n",
       " 68414: None,\n",
       " 68415: None,\n",
       " 68505: None,\n",
       " 68506: None,\n",
       " 68507: None,\n",
       " 68508: None,\n",
       " 69461: None,\n",
       " 69462: None,\n",
       " 69463: None,\n",
       " 69464: None,\n",
       " 69465: None,\n",
       " 69703: None,\n",
       " 69704: None,\n",
       " 69705: None,\n",
       " 69706: None,\n",
       " 69707: None,\n",
       " 69708: None,\n",
       " 69709: None,\n",
       " 69819: None,\n",
       " 69820: None,\n",
       " 69822: None,\n",
       " 69823: None,\n",
       " 69824: None,\n",
       " 69825: None,\n",
       " 69952: None,\n",
       " 69953: None,\n",
       " 69954: None,\n",
       " 69955: None,\n",
       " 70004: None,\n",
       " 70005: None,\n",
       " 70085: None,\n",
       " 70086: None,\n",
       " 70087: None,\n",
       " 70088: None,\n",
       " 70093: None,\n",
       " 70107: None,\n",
       " 70109: None,\n",
       " 70110: None,\n",
       " 70111: None,\n",
       " 70200: None,\n",
       " 70201: None,\n",
       " 70202: None,\n",
       " 70203: None,\n",
       " 70204: None,\n",
       " 70205: None,\n",
       " 70313: None,\n",
       " 70731: None,\n",
       " 70732: None,\n",
       " 70733: None,\n",
       " 70734: None,\n",
       " 70735: None,\n",
       " 70747: None,\n",
       " 70749: None,\n",
       " 70854: None,\n",
       " 71105: None,\n",
       " 71106: None,\n",
       " 71107: None,\n",
       " 71108: None,\n",
       " 71109: None,\n",
       " 71110: None,\n",
       " 71111: None,\n",
       " 71112: None,\n",
       " 71113: None,\n",
       " 71114: None,\n",
       " 71115: None,\n",
       " 71116: None,\n",
       " 71117: None,\n",
       " 71118: None,\n",
       " 71119: None,\n",
       " 71120: None,\n",
       " 71121: None,\n",
       " 71122: None,\n",
       " 71123: None,\n",
       " 71124: None,\n",
       " 71125: None,\n",
       " 71126: None,\n",
       " 71127: None,\n",
       " 71233: None,\n",
       " 71234: None,\n",
       " 71235: None,\n",
       " 71264: None,\n",
       " 71265: None,\n",
       " 71266: None,\n",
       " 71267: None,\n",
       " 71268: None,\n",
       " 71269: None,\n",
       " 71270: None,\n",
       " 71271: None,\n",
       " 71272: None,\n",
       " 71273: None,\n",
       " 71274: None,\n",
       " 71275: None,\n",
       " 71276: None,\n",
       " 71484: None,\n",
       " 71485: None,\n",
       " 71486: None,\n",
       " 71739: None,\n",
       " 72255: None,\n",
       " 72256: None,\n",
       " 72257: None,\n",
       " 72258: None,\n",
       " 72259: None,\n",
       " 72260: None,\n",
       " 72261: None,\n",
       " 72262: None,\n",
       " 72346: None,\n",
       " 72347: None,\n",
       " 72348: None,\n",
       " 72350: None,\n",
       " 72351: None,\n",
       " 72352: None,\n",
       " 72353: None,\n",
       " 72354: None,\n",
       " 72769: None,\n",
       " 72770: None,\n",
       " 72771: None,\n",
       " 72772: None,\n",
       " 72773: None,\n",
       " 72816: None,\n",
       " 72817: None,\n",
       " 73463: None,\n",
       " 73464: None,\n",
       " 74864: None,\n",
       " 74865: None,\n",
       " 74866: None,\n",
       " 74867: None,\n",
       " 74868: None,\n",
       " 92782: None,\n",
       " 92783: None,\n",
       " 92917: None,\n",
       " 92983: None,\n",
       " 92984: None,\n",
       " 92985: None,\n",
       " 92986: None,\n",
       " 92987: None,\n",
       " 92996: None,\n",
       " 93847: None,\n",
       " 93848: None,\n",
       " 93849: None,\n",
       " 93850: None,\n",
       " 113823: None,\n",
       " 121479: None,\n",
       " 121480: None,\n",
       " 121481: None,\n",
       " 121482: None,\n",
       " 121483: None,\n",
       " 125278: None,\n",
       " 125279: None}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字典中是unicode當key, 用None為值, 然後將字串裡所有字元轉為None(By translate method)\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:有時候標點符號給的資訊很重要，例如問句跟肯定句"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本的記號化(Tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sychen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "string = \"The science of today is the technology of tomorrow\"\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token轉為句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today\"\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords 移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sychen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_words=[\"i\", \"am\", \"going\", \"to\", \"go\", \"to\", \"the\", \"store\", \"and\", \"park\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: stopwords要求token都為小寫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞幹提取(Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "tokenizer_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']\n",
    "porter = PorterStemmer()\n",
    "[porter.stem(word) for word in tokenizer_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞類標記"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sychen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: PennTreeBank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "標籤 | 詞類\n",
    "----|-------\n",
    "NNP |Proper noun, singular\n",
    "NN|Noun, singular or mass\n",
    "RB|Adverb\n",
    "VBD|Verb, past tense\n",
    "VBG|Verb, gerund or present participle\n",
    "JJ|Adjective\n",
    "PRP|Personal pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word, tag in text_tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['PRP', 'VBP', 'VBG', 'DT', 'NN', 'IN', 'NN'],\n",
       " ['JJ', 'NN', 'VBZ', 'DT', 'JJ', 'NN'],\n",
       " ['NNP', 'NNP', 'VBZ', 'DT', 'JJ', 'NN']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = [\"I am eating a burrito for breakfast\", \"Political science is an amazing field\", \"San Francisco is an awesome city\"]\n",
    "tagged_tweets = []\n",
    "for tweet in tweets:\n",
    "        tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "        tagged_tweets.append([tag for word, tag in tweet_tag])\n",
    "tagged_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: 訓練tagger的方法，使用的語料庫是Brown Corpus，運用backoff n-gram tagger，n指的是預測該詞之詞類標籤時所要採計的前詞數目，首先我們用TrigraamTagger考慮前兩個詞來進行預測，若沒有則退後用BigramTagger考慮前一個詞，在沒有才用UnigramTagger考慮詞本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\sychen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "# 從Brown語料庫獲得文本，並將它拆解成句子\n",
    "sentence = brown.tagged_sents(categories='news')\n",
    "train = sentence[:4000]\n",
    "test = sentence[4000:]\n",
    "# 退後tagger\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 將文本編碼成詞袋(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_data = np.array(['I love Brazil. Brazil!', 'Sweden is best', 'Germany beats both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 產生詞袋特徵矩陣\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "bag_of_words # <3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 看特徵名稱\n",
    "count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beats</th>\n",
       "      <th>best</th>\n",
       "      <th>both</th>\n",
       "      <th>brazil</th>\n",
       "      <th>germany</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sweden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beats  best  both  brazil  germany  is  love  sweden\n",
       "0      0     0     0       2        0   0     1       0\n",
       "1      0     1     0       0        0   1     0       1\n",
       "2      1     0     1       0        1   0     0       0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(bag_of_words.toarray(), columns=count.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: bag-of-words model 會為每一個唯一詞輸出一個特徵，而像是上面的Brazil值為2是因為出現了兩次。\n",
    "#### 阿其實還是可以將每個特徵設定為兩個詞一個組合(2-gram)，甚至是三個詞(3-gram) 下面展示方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram = CountVectorizer(ngram_range=(1, 2), stop_words='english', vocabulary=['brazil'])\n",
    "bag = count_2gram.fit_transform(text_data)\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram.vocabulary_ # 出現兩次以上的詞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 詞的權重(TfidVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_data = np.array(['I love Brazil. Brazil!', 'Sweden is best', 'Germany beats both'])\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "feature_matrix\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'germany': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes: 一詞在文件中次數出現越多，該詞對該文件的重要性通常會比較高，我們稱為詞頻(term frequency, tf)\n",
    "#### 相對的，若一詞出現在許多文件中，該詞對個別文件的重要性就會降低，我們稱之為文件頻率(document frequency, df)\n",
    "#### 結合上述，我們可以為每一個詞指定一個代表該詞在一份文件中的重要性。簡單來說就是 tf * idf (tf乘df的倒數)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf-idf(t, d) = tf(t,d)\\times idf(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$idf(t)=log\\frac{1+n_d}{1+df(d,t)}+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$n_d是文件數量，df(d,t)則是詞，t，的文件頻率(即內涵該詞的文件數量)。scikit-learn會使用L2 norm來對tf-idf向量做normalize$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
